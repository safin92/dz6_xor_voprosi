{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safin92/dz6_xor_voprosi/blob/main/XOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e86c20",
      "metadata": {
        "id": "09e86c20"
      },
      "source": [
        "### Задача XOR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "953523e1",
      "metadata": {
        "id": "953523e1"
      },
      "source": [
        "# Задача: Реализация нейронной сети для решения проблемы XOR\n",
        "\n",
        "## Введение\n",
        "\n",
        "XOR (исключающее ИЛИ) - это логическая операция, которая возвращает истину только если входные значения различны. Таблица истинности для XOR:\n",
        "\n",
        "| A | B | A XOR B |\n",
        "|:-:|:-:|:-------:|\n",
        "| 0 | 0 |    0    |\n",
        "| 0 | 1 |    1    |\n",
        "| 1 | 0 |    1    |\n",
        "| 1 | 1 |    0    |\n",
        "\n",
        "Эта задача не может быть решена с помощью линейной модели, поэтому она часто используется для демонстрации возможностей нейронных сетей.\n",
        "\n",
        "## Задание\n",
        "\n",
        "Ваша задача - реализовать двухслойную нейронную сеть для решения проблемы XOR.\n",
        "\n",
        "### Архитектура сети:\n",
        "\n",
        "- Входной слой: 2 нейрона (для A и B)\n",
        "- Скрытый слой: 4 нейрона\n",
        "- Выходной слой: 1 нейрон\n",
        "\n",
        "![Архитектура нейронной сети для XOR](Tutorial-And.jpg)\n",
        "\n",
        "## Инструкции:\n",
        "\n",
        "1. Изучите предоставленный код класса `NeuralNetwork`.\n",
        "2. Заполните все места, отмеченные `TODO`, используя предоставленные подсказки.\n",
        "3. Используйте функции NumPy (`np.dot`, `np.random.uniform`, `np.sum`) для выполнения необходимых вычислений.\n",
        "4. Реализуйте прямое и обратное распространение, а также обновление весов.\n",
        "5. Обучите сеть на предоставленных данных XOR.\n",
        "6. Протестируйте сеть и выведите результаты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "274ade8a",
      "metadata": {
        "id": "274ade8a"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "25c459f9",
      "metadata": {
        "id": "25c459f9"
      },
      "outputs": [],
      "source": [
        "# Класс нейронной сети с одним скрытым слоем\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Инициализация параметров сети\n",
        "        self.input_size = input_size      # Размер входного слоя (количество нейронов (признаков))\n",
        "        self.hidden_size = hidden_size    # Количество нейронов в скрытом слое\n",
        "        self.output_size = output_size    # Размер выходного слоя\n",
        "\n",
        "        # Шаг 1: Инициализация весов и смещений\n",
        "        # Весовые коэффициенты и смещения инициализируются случайным образом для разрыва симметрии в начальных значениях\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size)) # Веса между входным и скрытым слоем\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))              # Смещения для скрытого слоя\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size)) # Веса между скрытым и выходным слоем\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))              # Смещения для выходного слоя\n",
        "\n",
        "    # Шаг 2: Функция активации - Сигмоида\n",
        "    # Используется для нелинейности в нейронной сети\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # Шаг 3: Производная сигмоиды\n",
        "    # Используется для вычисления градиентов при обратном распространении ошибки\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # Шаг 4: Прямое распространение (Forward Propagation)\n",
        "    # Вычисление выходных значений сети на основе входных данных\n",
        "    def forward(self, X):\n",
        "        # Входной слой -> Скрытый слой\n",
        "        # Применяем веса и смещения, затем функцию активации\n",
        "        self.hidden_layer = self.sigmoid(np.dot(X, self.hidden_weights) + self.hidden_bias)\n",
        "\n",
        "        # Скрытый слой -> Выходной слой\n",
        "        # Применяем веса и смещения, затем функцию активации\n",
        "        self.output_layer = self.sigmoid(np.dot(self.hidden_layer, self.output_weights) + self.output_bias)\n",
        "\n",
        "        # Возвращаем выходное значение\n",
        "        return self.output_layer\n",
        "\n",
        "    # Шаг 5: Обратное распространение ошибки (Backward Propagation)\n",
        "    # Обновление весов и смещений на основе ошибки предсказания\n",
        "    def backward(self, X, y, output):\n",
        "        # Шаг 5.1: Вычисление ошибки выходного слоя\n",
        "        # Разница между фактическим и предсказанным значением\n",
        "        error = y - output\n",
        "\n",
        "        # Шаг 5.2: Вычисление градиента выходного слоя\n",
        "        # Умножаем ошибку на производную функции активации\n",
        "        d_output = error * self.sigmoid_derivative(output)\n",
        "\n",
        "        # Шаг 5.3: Распространение ошибки на скрытый слой\n",
        "        # Умножаем градиенты выходного слоя на веса выходного слоя\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # Шаг 5.4: Вычисление градиента скрытого слоя\n",
        "        # Умножаем ошибку скрытого слоя на производную функции активации\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer)\n",
        "\n",
        "        # Шаг 5.5: Обновление весов и смещений\n",
        "        # Используем градиентный спуск для корректировки весов и смещений\n",
        "        # Выходной слой\n",
        "        self.output_weights += np.dot(self.hidden_layer.T, d_output) * self.learning_rate\n",
        "        self.output_bias += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "        # Скрытый слой\n",
        "        self.hidden_weights += np.dot(X.T, d_hidden_layer) * self.learning_rate\n",
        "        self.hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "    # Шаг 6: Обучение нейронной сети\n",
        "    # Повторение этапов прямого и обратного распространения для каждой эпохи\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate  # Устанавливаем скорость обучения\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)        # Прямое распространение\n",
        "            self.backward(X, y, output)     # Обратное распространение\n",
        "\n",
        "    # Шаг 7: Предсказание\n",
        "    # Вычисление выхода для новых данных без обучения\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a3cec421",
      "metadata": {
        "id": "a3cec421"
      },
      "outputs": [],
      "source": [
        "# Наш \"датасет\" для обучения\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1744a778",
      "metadata": {
        "id": "1744a778"
      },
      "outputs": [],
      "source": [
        "# Инициализация нейронной сети\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "#Цикл обучения\n",
        "nn.train(X, y, epochs = 1000, learning_rate=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0114af1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0114af1f",
        "outputId": "e176640b-3ee1-45b9-bf23-62c0265ca93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выходные данные после обучения:\n",
            "[[0.99997724]\n",
            " [0.99999443]\n",
            " [0.99999296]\n",
            " [0.99999706]]\n",
            "\n",
            "Точность: 50.0%\n"
          ]
        }
      ],
      "source": [
        "predictions = nn.predict(X)\n",
        "print(\"Выходные данные после обучения:\")\n",
        "print(predictions)\n",
        "\n",
        "# Подсказка: Используйте np.mean для вычисления среднего значения\n",
        "# np.mean вычисляет среднее арифметическое элементов массива\n",
        "accuracy = np.mean(np.round(predictions) == y)\n",
        "print(f\"\\nТочность: {accuracy * 100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jlxFefldpazc"
      },
      "id": "jlxFefldpazc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Как называется алгоритм нейронный сетей.**\n",
        "\n",
        "Градиентный спуск (или обртаное распространение ошибки). Элементы алгоритма включают в себя:\n",
        "  \n",
        "  1) Прямое распространение: данные проходят через сеть, от входного слоя к выходному - каждый нейрон взвешивает сумму входов, добавлет смещение и пропускает результат через функцию активации - на выходе сеть дает предсказание.\n",
        "  \n",
        "  2) Вычисление ошибки: сравниваем предсказание сети с правильным ответом и вычисляем ошибку (MSE, крос-энтропия).\n",
        "  \n",
        "  3) Обратное распространение ошибки: ошибка распространяется назад по сети, начиная с выходного слоя - вычисляются градиенты (производные функции по всем весам).\n",
        "  \n",
        "  4) Обновление весов: градиентный спуск корректирует веса так, чтобы уменьшить ошибку - вес обновляется по формуле (новый вес = старый вес - скорость обучения*градиент ошибки по весу.\n",
        "\n",
        "\n",
        "**2. По какому методу градиентного спуска реализован алгоритм?**\n",
        "\n",
        "Методы градиентного спуска:\n",
        "  \n",
        "  1) Полный (Batch) градиентный спуск (Batch Gradient Descent, BGD)\n",
        "\n",
        "Обновляет веса раз в эпоху (полный проход посети), используя всю выборку данных.\n",
        "Точный, но медленный и требует много памяти.\n",
        "Хорош для гладких функций (например, в линейной регрессии).\n",
        "\n",
        "  2) Стохастический градиентный спуск (Stochastic Gradient Descent, SGD)\n",
        "\n",
        "Обновляет веса после каждого примера (пример - отдельный вход, один объект данных , который подается на вход нейросети).\n",
        "Быстрее, но из-за случайности обновлений может колебаться и не всегда сходится к оптимальному минимуму.\n",
        "Хорош для онлайн-обучения.\n",
        "\n",
        "  3) Мини-батч градиентный спуск (Mini-Batch Gradient Descent, MBGD)\n",
        "\n",
        "Компромисс между BGD и SGD. Разбивает выборку на маленькие группы (батчи) и обновляет веса после обработки каждого батча.\n",
        "Быстрее, чем BGD, и менее шумный, чем SGD.\n",
        "\n",
        "  4) Adagrad (Adaptive Gradient Descent)\n",
        "\n",
        "Разные параметры обучаются с разной скоростью.\n",
        "Хорошо работает для разреженных данных.\n",
        "Проблема: скорость обучения со временем становится слишком маленькой.\n",
        "\n",
        "  5) RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "Улучшает Adagrad: добавляет затухающее среднее квадратов градиентов.\n",
        "Хорошо работает в глубоких сетях.\n",
        "\n",
        "  6) Adam (Adaptive Moment Estimation) – самый популярный\n",
        "\n",
        "Комбинирует Momentum и RMSprop.\n",
        "Самый распространенный метод в нейросетях.\n",
        "Автоматически регулирует скорость обучения для каждого параметра.\n",
        "\n",
        "Какой метод выбрать?\n",
        "Если данных мало → BGD (полный градиентный спуск).\n",
        "Если данных много → Mini-Batch или Adam.\n",
        "Для онлайн-обучения → SGD.\n",
        "Для глубоких сетей → Adam или RMSprop.\n",
        "\n",
        "В коде реализован полный (Batch) градиентный спуск, так как веса обновляются после обработки всего набора данных.\n",
        "\n",
        "**3. Как подбирать Learning_rate**\n",
        "\n",
        "Скорость обучения - коэффициент, который определяет, насколько сильно обновляются веса нейросети на каждом шаге обучения.\n",
        "\n",
        "Выбор learning_rate критически важен:\n",
        "\n",
        "Слишком маленький α → обучение идет очень медленно, можно застрять в локальном минимуме.\n",
        "Слишком большой α → веса обновляются слишком резко, может начаться \"скачкообразное\" обучение или сеть вообще не сойдется.\n",
        "\n",
        "Методы определения коэффициента:\n",
        "\n",
        "1) Метод проб и ошибок (эмпирический поиск)\n",
        "Простейший способ — начать с небольшого значения (0.01 или 0.001) и наблюдать за изменением ошибки.\n",
        "\n",
        "Если ошибка уменьшается медленно → попробуй увеличить α.\n",
        "Если ошибка скачет или растет → уменьшай α.\n",
        "\n",
        "Пример:\n",
        "\n",
        "nn.train(X, y, epochs=1000, learning_rate=0.01)\n",
        "\n",
        "Начни с 0.01. Если градиент \"скачет\" → уменьши α (0.001).\n",
        "Если сходится медленно → попробуй 0.05 или 0.1.\n",
        "\n",
        "2) Метод логарифмического поиска (поиск по степеням 10)\n",
        "Попробуй разные значения α в диапазоне от 1e-5 до 1e-1 (от 0.00001 до 0.1):\n",
        "\n",
        "    for lr in [0.00001, 0.0001, 0.001, 0.01, 0.1]:\n",
        "\n",
        "    print(f\"Тестируем learning_rate = {lr}\")\n",
        "\n",
        "    nn.train(X, y, epochs=1000, learning_rate=lr)\n",
        "    \n",
        "Если α = 0.1 работает хорошо, попробуй 0.05 или 0.2 для уточнения.\n",
        "\n",
        "3) Адаптивные методы (Adam, RMSprop, Adagrad)\n",
        "Можно использовать автоматическую подстройку learning_rate:\n",
        "\n",
        "Adam — один из лучших оптимизаторов (сочетает Momentum + RMSprop).\n",
        "RMSprop — хорошо работает на сложных задачах.\n",
        "Adagrad — подходит, если данные сильно различаются по масштабам.\n",
        "\n",
        "Пример с Adam в Keras:\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "\n",
        "4) Эксперимент с \"learning rate decay\" (уменьшение скорости со временем)\n",
        "Иногда обучение сначала требует большого α, но потом его лучше уменьшить. Это называется \"learning rate decay\" (затухание скорости обучения).\n",
        "\n",
        "Пример:\n",
        "\n",
        "    initial_lr = 0.1\n",
        "    decay_rate = 0.01\n",
        "    lr = initial_lr / (1 + decay_rate * epoch)  # Уменьшаем lr по мере роста эпох\n",
        "\n",
        "Чем дальше обучение, тем меньше шаги обновления весов, что помогает точнее настроить сеть.\n",
        "\n",
        "5) Визуальный анализ (график ошибки)\n",
        "Можно строить график ошибки (loss function) в зависимости от эпох.\n",
        "\n",
        "Пример:\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    losses = []\n",
        "    for epoch in range(1000):\n",
        "      output = nn.forward(X)\n",
        "      loss = np.mean((y - output) ** 2)  # Среднеквадратичная ошибка (MSE)\n",
        "      losses.append(loss)\n",
        "    nn.backward(X, y, output)\n",
        "\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel(\"Эпохи\")\n",
        "    plt.ylabel(\"Ошибка (loss)\")\n",
        "    plt.show()\n",
        "\n",
        "Если кривая падает медленно → увеличь α.\n",
        "Если скачет или растет → уменьши α.\n",
        "\n",
        "Вывод:\n",
        "Начинай с α = 0.01 или 0.001, тестируй.\n",
        "Используй логарифмический поиск (0.00001 → 0.0001 → 0.001 → 0.01 → 0.1).\n",
        "Для сложных моделей попробуй Adam или RMSprop вместо стандартного градиентного спуска.\n",
        "Можно уменьшать α со временем (learning rate decay).\n",
        "Строй графики ошибки, чтобы видеть, как α влияет на обучение.\n",
        "Лучший learning_rate зависит от данных! 🚀\n",
        "\n",
        "**4. Что такое np.random.uniform?**\n",
        "\n",
        "np.random — это модуль в NumPy, который содержит множество функций для генерации случайных чисел.\n",
        "\n",
        "np.random.uniform(low, high, size) — это конкретная функция внутри np.random, которая генерирует равномерно распределенные числа в заданном диапазоне [low, high]."
      ],
      "metadata": {
        "id": "cQuICl6ZgkA9"
      },
      "id": "cQuICl6ZgkA9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5) веса задать нулями и смешения тоже**\n",
        "\n",
        "Ниже представлен код с начальными весами и смещениями равными 0.\n"
      ],
      "metadata": {
        "id": "lNIiLFxI_D2k"
      },
      "id": "lNIiLFxI_D2k"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Класс нейронной сети с одним скрытым слоем\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Инициализация параметров сети\n",
        "        self.input_size = input_size      # Размер входного слоя (количество нейронов (признаков))\n",
        "        self.hidden_size = hidden_size    # Количество нейронов в скрытом слое\n",
        "        self.output_size = output_size    # Размер выходного слоя\n",
        "\n",
        "        # Шаг 1: Инициализация весов и смещений\n",
        "        # Весовые коэффициенты и смещения инициализируются случайным образом для разрыва симметрии в начальных значениях\n",
        "        self.hidden_weights = np.zeros((input_size, hidden_size)) # Веса между входным и скрытым слоем = 0\n",
        "        self.hidden_bias = np.zeros((1, hidden_size))              # Смещения для скрытого слоя = 0\n",
        "        self.output_weights = np.zeros((hidden_size, output_size)) # Веса между скрытым и выходным слоем = 0\n",
        "        self.output_bias = np.zeros((1, output_size))              # Смещения для выходного слоя = 0\n",
        "\n",
        "    # Шаг 2: Функция активации - Сигмоида\n",
        "    # Используется для нелинейности в нейронной сети\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # Шаг 3: Производная сигмоиды\n",
        "    # Используется для вычисления градиентов при обратном распространении ошибки\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # Шаг 4: Прямое распространение (Forward Propagation)\n",
        "    # Вычисление выходных значений сети на основе входных данных\n",
        "    def forward(self, X):\n",
        "        # Входной слой -> Скрытый слой\n",
        "        # Применяем веса и смещения, затем функцию активации\n",
        "        self.hidden_layer = self.sigmoid(np.dot(X, self.hidden_weights) + self.hidden_bias)\n",
        "\n",
        "        # Скрытый слой -> Выходной слой\n",
        "        # Применяем веса и смещения, затем функцию активации\n",
        "        self.output_layer = self.sigmoid(np.dot(self.hidden_layer, self.output_weights) + self.output_bias)\n",
        "\n",
        "        # Возвращаем выходное значение\n",
        "        return self.output_layer\n",
        "\n",
        "    # Шаг 5: Обратное распространение ошибки (Backward Propagation)\n",
        "    # Обновление весов и смещений на основе ошибки предсказания\n",
        "    def backward(self, X, y, output):\n",
        "        # Шаг 5.1: Вычисление ошибки выходного слоя\n",
        "        # Разница между фактическим и предсказанным значением\n",
        "        error = y - output\n",
        "\n",
        "        # Шаг 5.2: Вычисление градиента выходного слоя\n",
        "        # Умножаем ошибку на производную функции активации\n",
        "        d_output = error * self.sigmoid_derivative(output)\n",
        "\n",
        "        # Шаг 5.3: Распространение ошибки на скрытый слой\n",
        "        # Умножаем градиенты выходного слоя на веса выходного слоя\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # Шаг 5.4: Вычисление градиента скрытого слоя\n",
        "        # Умножаем ошибку скрытого слоя на производную функции активации\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer)\n",
        "\n",
        "        # Шаг 5.5: Обновление весов и смещений\n",
        "        # Используем градиентный спуск для корректировки весов и смещений\n",
        "        # Выходной слой\n",
        "        self.output_weights += np.dot(self.hidden_layer.T, d_output) * self.learning_rate\n",
        "        self.output_bias += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "        # Скрытый слой\n",
        "        self.hidden_weights += np.dot(X.T, d_hidden_layer) * self.learning_rate\n",
        "        self.hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "    # Шаг 6: Обучение нейронной сети\n",
        "    # Повторение этапов прямого и обратного распространения для каждой эпохи\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate  # Устанавливаем скорость обучения\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)        # Прямое распространение\n",
        "            self.backward(X, y, output)     # Обратное распространение\n",
        "\n",
        "    # Шаг 7: Предсказание\n",
        "    # Вычисление выхода для новых данных без обучения\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# Наш \"датасет\" для обучения\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Инициализация нейронной сети\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "#Цикл обучения\n",
        "nn.train(X, y, epochs = 1000, learning_rate=10)\n",
        "\n",
        "predictions = nn.predict(X)\n",
        "print(\"Выходные данные после обучения:\")\n",
        "print(predictions)\n",
        "\n",
        "# Подсказка: Используйте np.mean для вычисления среднего значения\n",
        "# np.mean вычисляет среднее арифметическое элементов массива\n",
        "accuracy = np.mean(np.round(predictions) == y)\n",
        "print(f\"\\nТочность: {accuracy * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy4MepXN_LAc",
        "outputId": "4d9b04ae-3fb3-476f-c453-ca774890cfd1"
      },
      "id": "Yy4MepXN_LAc",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выходные данные после обучения:\n",
            "[[0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]]\n",
            "\n",
            "Точность: 50.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При задании различных смещений нейросеть продолжает корректную раьботу, если задать и веса и смещения нулями, то нейросеть не обучается.\n",
        "\n",
        "1) Все нейроны в одном слое получают одинаковые градиенты.\n",
        "Во время обратного распространения (backpropagation) градиенты вычисляются через веса. Если все веса изначально нули, то градиенты для всех нейронов будут одинаковыми.\n",
        "Это приводит к разрыву симметрии — все нейроны обновляются одинаково, и сеть не может учиться.\n",
        "\n",
        "2) Сеть остаётся линейной.\n",
        "Без случайной инициализации веса остаются одинаковыми, а значит, каждый слой выполняет линейное преобразование.\n",
        "Линейная модель не способна обучаться сложным зависимостям (а нейросеть нужна именно для нелинейного обучения)."
      ],
      "metadata": {
        "id": "TWxHfKMmkcPu"
      },
      "id": "TWxHfKMmkcPu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. В шаге 5.5 используется \"+=\", почему не \"-+\"?**\n",
        "\n",
        "Классическая формула обновления весов выглядит так:\n",
        "\n",
        "𝑊 = 𝑊 − 𝜂⋅∂𝐿\n",
        "\n",
        "где:\n",
        "\n",
        "𝑊 — веса,\n",
        "𝜂 (learning rate) — скорость обучения,\n",
        "∂𝐿 — градиент функции ошибки (L — loss).\n",
        "Таким образом, обычно мы вычитаем градиент (-=), чтобы двигаться в сторону убывания ошибки.\n",
        "\n",
        "Правильно использовать -=, но в коде уже имеется ошибка, которая позволяет нам использовать плюс - error = y - output. Чтобы использовать -=, необходимо заменить формулу error = y - output на error = output - y\n",
        "\n"
      ],
      "metadata": {
        "id": "nbblFcts_Jj9"
      },
      "id": "nbblFcts_Jj9"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Класс нейронной сети с одним скрытым слоем\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Инициализация параметров сети\n",
        "        self.input_size = input_size      # Размер входного слоя (количество нейронов (признаков))\n",
        "        self.hidden_size = hidden_size    # Количество нейронов в скрытом слое\n",
        "        self.output_size = output_size    # Размер выходного слоя\n",
        "\n",
        "        # Шаг 1: Инициализация весов и смещений\n",
        "        # Весовые коэффициенты и смещения инициализируются случайным образом для разрыва симметрии в начальных значениях\n",
        "        self.hidden_weights = np.random.uniform(size = (input_size, hidden_size)) # Веса между входным и скрытым слоем\n",
        "        self.hidden_bias = np.random.uniform(size = (1, hidden_size))              # Смещения для скрытого слоя\n",
        "        self.output_weights = np.random.uniform(size = (hidden_size, output_size)) # Веса между скрытым и выходным слоем\n",
        "        self.output_bias = np.random.uniform(size = (1, output_size))              # Смещения для выходного слоя\n",
        "\n",
        "    # Шаг 2: Функция активации - Сигмоида\n",
        "    # Используется для нелинейности в нейронной сети\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # Шаг 3: Производная сигмоиды\n",
        "    # Используется для вычисления градиентов при обратном распространении ошибки\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # Шаг 4: Прямое распространение (Forward Propagation)\n",
        "    # Вычисление выходных значений сети на основе входных данных\n",
        "    def forward(self, X):\n",
        "        # Входной слой -> Скрытый слой\n",
        "        # Применяем веса и смещения, затем функцию активации\n",
        "        self.hidden_layer = self.sigmoid(np.dot(X, self.hidden_weights) + self.hidden_bias)\n",
        "\n",
        "        # Скрытый слой -> Выходной слой\n",
        "        # Применяем веса и смещения, затем функцию активации\n",
        "        self.output_layer = self.sigmoid(np.dot(self.hidden_layer, self.output_weights) + self.output_bias)\n",
        "\n",
        "        # Возвращаем выходное значение\n",
        "        return self.output_layer\n",
        "\n",
        "    # Шаг 5: Обратное распространение ошибки (Backward Propagation)\n",
        "    # Обновление весов и смещений на основе ошибки предсказания\n",
        "    def backward(self, X, y, output):\n",
        "        # Шаг 5.1: Вычисление ошибки выходного слоя\n",
        "        # Разница между фактическим и предсказанным значением\n",
        "        error = output - y\n",
        "\n",
        "        # Шаг 5.2: Вычисление градиента выходного слоя\n",
        "        # Умножаем ошибку на производную функции активации\n",
        "        d_output = error * self.sigmoid_derivative(output)\n",
        "\n",
        "        # Шаг 5.3: Распространение ошибки на скрытый слой\n",
        "        # Умножаем градиенты выходного слоя на веса выходного слоя\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # Шаг 5.4: Вычисление градиента скрытого слоя\n",
        "        # Умножаем ошибку скрытого слоя на производную функции активации\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer)\n",
        "\n",
        "        # Шаг 5.5: Обновление весов и смещений\n",
        "        # Используем градиентный спуск для корректировки весов и смещений\n",
        "        # Выходной слой\n",
        "        self.output_weights -= np.dot(self.hidden_layer.T, d_output) * self.learning_rate\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "        # Скрытый слой\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "    # Шаг 6: Обучение нейронной сети\n",
        "    # Повторение этапов прямого и обратного распространения для каждой эпохи\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate  # Устанавливаем скорость обучения\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)        # Прямое распространение\n",
        "            self.backward(X, y, output)     # Обратное распространение\n",
        "\n",
        "    # Шаг 7: Предсказание\n",
        "    # Вычисление выхода для новых данных без обучения\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# Наш \"датасет\" для обучения\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Инициализация нейронной сети\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "#Цикл обучения\n",
        "nn.train(X, y, epochs = 1000, learning_rate=10)\n",
        "\n",
        "predictions = nn.predict(X)\n",
        "print(\"Выходные данные после обучения:\")\n",
        "print(predictions)\n",
        "\n",
        "# Подсказка: Используйте np.mean для вычисления среднего значения\n",
        "# np.mean вычисляет среднее арифметическое элементов массива\n",
        "accuracy = np.mean(np.round(predictions) == y)\n",
        "print(f\"\\nТочность: {accuracy * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsewcjuOt1VJ",
        "outputId": "9885188d-f140-4f07-b00a-ab26ed254dac"
      },
      "id": "jsewcjuOt1VJ",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выходные данные после обучения:\n",
            "[[0.00979599]\n",
            " [0.98621319]\n",
            " [0.99202569]\n",
            " [0.01480533]]\n",
            "\n",
            "Точность: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Попробовать софтмакс вместо сигмоиды, обучить софтмаксом**\n",
        "\n",
        "Вариант полного использования Softmax"
      ],
      "metadata": {
        "id": "rqeCLLOVtpJ-"
      },
      "id": "rqeCLLOVtpJ-"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Инициализация параметров сети\n",
        "        self.input_size = input_size      # Размер входного слоя\n",
        "        self.hidden_size = hidden_size    # Количество нейронов в скрытом слое\n",
        "        self.output_size = output_size    # Размер выходного слоя\n",
        "\n",
        "        # Инициализация весов и смещений\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size))  # Веса между входным и скрытым слоем\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))              # Смещения для скрытого слоя\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size)) # Веса между скрытым и выходным слоем\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))              # Смещения для выходного слоя\n",
        "\n",
        "    # Функция активации - Softmax\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Нормализация для стабильности\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    # Производная Softmax\n",
        "    def softmax_derivative(self, x):\n",
        "        s = x.reshape(-1, 1)\n",
        "        return np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    # Прямое распространение (Forward Propagation)\n",
        "    def forward(self, X):\n",
        "        # Входной слой -> Скрытый слой\n",
        "        self.hidden_layer_input = np.dot(X, self.hidden_weights) + self.hidden_bias\n",
        "        self.hidden_layer_output = self.softmax(self.hidden_layer_input)\n",
        "\n",
        "        # Скрытый слой -> Выходной слой\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.output_weights) + self.output_bias\n",
        "        self.output_layer_output = self.softmax(self.output_layer_input)\n",
        "\n",
        "        return self.output_layer_output\n",
        "\n",
        "    # Обратное распространение ошибки (Backward Propagation)\n",
        "    def backward(self, X, y, output):\n",
        "        batch_size = X.shape[0]\n",
        "\n",
        "        # Ошибка выходного слоя\n",
        "        error = output - y\n",
        "\n",
        "        # Градиент выходного слоя\n",
        "        d_output = np.zeros_like(output)\n",
        "        for i in range(batch_size):\n",
        "            d_output[i] = np.dot(self.softmax_derivative(output[i]), error[i])\n",
        "\n",
        "        # Ошибка скрытого слоя\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # Градиент скрытого слоя\n",
        "        d_hidden_layer = np.zeros_like(self.hidden_layer_output)\n",
        "        for i in range(batch_size):\n",
        "            d_hidden_layer[i] = np.dot(self.softmax_derivative(self.hidden_layer_output[i]), error_hidden_layer[i])\n",
        "\n",
        "        # Обновление весов и смещений\n",
        "        self.output_weights -= np.dot(self.hidden_layer_output.T, d_output) * self.learning_rate / batch_size\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate / batch_size\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "    # Обучение нейронной сети\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate  # Устанавливаем скорость обучения\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)        # Прямое распространение\n",
        "            self.backward(X, y, output)     # Обратное распространение\n",
        "\n",
        "    # Предсказание\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# Датасет для обучения\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])  # One-hot encoding для многоклассовой классификации\n",
        "\n",
        "# Инициализация нейронной сети\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=2)\n",
        "\n",
        "# Цикл обучения\n",
        "nn.train(X, y, epochs=1000, learning_rate=10)\n",
        "\n",
        "# Предсказание\n",
        "predictions = nn.predict(X)\n",
        "print(\"Выходные данные после обучения:\")\n",
        "print(predictions)\n",
        "\n",
        "# Точность\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(y, axis=1)\n",
        "accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f\"\\nТочность: {accuracy * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06fHwxMD0Emf",
        "outputId": "c66ed3ff-e742-452c-845e-ce6f4cd0378a"
      },
      "id": "06fHwxMD0Emf",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выходные данные после обучения:\n",
            "[[0.99344673 0.00655327]\n",
            " [0.00792608 0.99207392]\n",
            " [0.00782483 0.99217517]\n",
            " [0.99117347 0.00882653]]\n",
            "\n",
            "Точность: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax на выходном слое"
      ],
      "metadata": {
        "id": "nnbVOzyN6b8X"
      },
      "id": "nnbVOzyN6b8X"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Инициализация параметров сети\n",
        "        self.input_size = input_size      # Размер входного слоя\n",
        "        self.hidden_size = hidden_size    # Количество нейронов в скрытом слое\n",
        "        self.output_size = output_size    # Размер выходного слоя\n",
        "\n",
        "        # Инициализация весов и смещений\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size))  # Веса между входным и скрытым слоем\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))              # Смещения для скрытого слоя\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size)) # Веса между скрытым и выходным слоем\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))              # Смещения для выходного слоя\n",
        "\n",
        "    # Функция активации - Softmax\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Нормализация для стабильности\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    # Функция активации - Сигмоида\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # Производная сигмоиды\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # Прямое распространение (Forward Propagation)\n",
        "    def forward(self, X):\n",
        "        # Входной слой -> Скрытый слой\n",
        "        self.hidden_layer_input = np.dot(X, self.hidden_weights) + self.hidden_bias\n",
        "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        # Скрытый слой -> Выходной слой\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.output_weights) + self.output_bias\n",
        "        self.output_layer_output = self.softmax(self.output_layer_input)\n",
        "\n",
        "        return self.output_layer_output\n",
        "\n",
        "    # Обратное распространение ошибки (Backward Propagation)\n",
        "    def backward(self, X, y, output):\n",
        "        batch_size = X.shape[0]\n",
        "\n",
        "        # Ошибка выходного слоя\n",
        "        error = output - y\n",
        "\n",
        "        # Градиент выходного слоя\n",
        "        d_output = error  # Для Softmax градиент равен разнице между предсказанием и целевым значением\n",
        "\n",
        "        # Ошибка скрытого слоя\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # Градиент скрытого слоя\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        # Обновление весов и смещений\n",
        "        self.output_weights -= np.dot(self.hidden_layer_output.T, d_output) * self.learning_rate / batch_size\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate / batch_size\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "    # Обучение нейронной сети\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate  # Устанавливаем скорость обучения\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)        # Прямое распространение\n",
        "            self.backward(X, y, output)     # Обратное распространение\n",
        "\n",
        "    # Предсказание\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# Датасет для обучения\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])  # One-hot encoding для многоклассовой классификации\n",
        "\n",
        "# Инициализация нейронной сети\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=2)\n",
        "\n",
        "# Цикл обучения\n",
        "nn.train(X, y, epochs=1000, learning_rate=10)\n",
        "\n",
        "# Предсказание\n",
        "predictions = nn.predict(X)\n",
        "print(\"Выходные данные после обучения:\")\n",
        "print(predictions)\n",
        "\n",
        "# Точность\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(y, axis=1)\n",
        "accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f\"\\nТочность: {accuracy * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN6jWLUk6geK",
        "outputId": "ae88c44d-02e7-48bc-8f03-ff7ccc8bf6f8"
      },
      "id": "GN6jWLUk6geK",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выходные данные после обучения:\n",
            "[[9.99177783e-01 8.22216852e-04]\n",
            " [5.13253049e-04 9.99486747e-01]\n",
            " [6.44733945e-04 9.99355266e-01]\n",
            " [9.99374725e-01 6.25274536e-04]]\n",
            "\n",
            "Точность: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сравнение всех вариантов по точности и производительности"
      ],
      "metadata": {
        "id": "1RrHmofU_rYL"
      },
      "id": "1RrHmofU_rYL"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Класс с Softmax на всех слоях\n",
        "class NeuralNetworkSoftmaxAllLayers:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size))\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size))\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def softmax_derivative(self, x):\n",
        "        s = x.reshape(-1, 1)\n",
        "        return np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden_layer_input = np.dot(X, self.hidden_weights) + self.hidden_bias\n",
        "        self.hidden_layer_output = self.softmax(self.hidden_layer_input)\n",
        "\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.output_weights) + self.output_bias\n",
        "        self.output_layer_output = self.softmax(self.output_layer_input)\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        batch_size = X.shape[0]\n",
        "        error = output - y\n",
        "        d_output = error\n",
        "\n",
        "        # Градиент для выходного слоя\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # Градиент для скрытого слоя (вычисляем для каждого примера отдельно)\n",
        "        d_hidden_layer = np.zeros_like(self.hidden_layer_output)\n",
        "        for i in range(batch_size):\n",
        "            softmax_deriv = self.softmax_derivative(self.hidden_layer_output[i])\n",
        "            d_hidden_layer[i] = np.dot(error_hidden_layer[i], softmax_deriv)\n",
        "\n",
        "        # Обновление весов и смещений\n",
        "        self.output_weights -= np.dot(self.hidden_layer_output.T, d_output) * self.learning_rate / batch_size\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate / batch_size\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "\n",
        "# Класс с Softmax только на выходном слое\n",
        "class NeuralNetworkSoftmaxOutputLayer:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size))\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size))\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden_layer_input = np.dot(X, self.hidden_weights) + self.hidden_bias\n",
        "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.output_weights) + self.output_bias\n",
        "        self.output_layer_output = self.softmax(self.output_layer_input)\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        batch_size = X.shape[0]\n",
        "        error = output - y\n",
        "        d_output = error\n",
        "\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        self.output_weights -= np.dot(self.hidden_layer_output.T, d_output) * self.learning_rate / batch_size\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate / batch_size\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "\n",
        "# Класс с сигмоидой на всех слоях\n",
        "class NeuralNetworkSigmoidAllLayers:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size))\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size))\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden_layer_input = np.dot(X, self.hidden_weights) + self.hidden_bias\n",
        "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.output_weights) + self.output_bias\n",
        "        self.output_layer_output = self.sigmoid(self.output_layer_input)\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        batch_size = X.shape[0]\n",
        "        error = output - y\n",
        "        d_output = error * self.sigmoid_derivative(output)\n",
        "\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        self.output_weights -= np.dot(self.hidden_layer_output.T, d_output) * self.learning_rate / batch_size\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate / batch_size\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "\n",
        "# Датасет для обучения\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])  # One-hot encoding\n",
        "\n",
        "# Параметры обучения\n",
        "epochs = 1000\n",
        "learning_rate = 10\n",
        "\n",
        "# Создание сетей\n",
        "nn_softmax_all = NeuralNetworkSoftmaxAllLayers(input_size=2, hidden_size=4, output_size=2)\n",
        "nn_softmax_output = NeuralNetworkSoftmaxOutputLayer(input_size=2, hidden_size=4, output_size=2)\n",
        "nn_sigmoid_all = NeuralNetworkSigmoidAllLayers(input_size=2, hidden_size=4, output_size=2)\n",
        "\n",
        "# Обучение и измерение времени\n",
        "start_time = time.time()\n",
        "nn_softmax_all.train(X, y, epochs, learning_rate)\n",
        "time_softmax_all = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "nn_softmax_output.train(X, y, epochs, learning_rate)\n",
        "time_softmax_output = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "nn_sigmoid_all.train(X, y, epochs, learning_rate)\n",
        "time_sigmoid_all = time.time() - start_time\n",
        "\n",
        "# Предсказание и точность\n",
        "predictions_all = nn_softmax_all.predict(X)\n",
        "predictions_output = nn_softmax_output.predict(X)\n",
        "predictions_sigmoid_all = nn_sigmoid_all.predict(X)\n",
        "\n",
        "accuracy_all = np.mean(np.argmax(predictions_all, axis=1) == np.argmax(y, axis=1))\n",
        "accuracy_output = np.mean(np.argmax(predictions_output, axis=1) == np.argmax(y, axis=1))\n",
        "accuracy_sigmoid_all = np.mean(np.argmax(predictions_sigmoid_all, axis=1) == np.argmax(y, axis=1))\n",
        "\n",
        "# Вывод результатов\n",
        "print(f\"Точность (Softmax на всех слоях): {accuracy_all * 100:.2f}%\")\n",
        "print(f\"Точность (Softmax только на выходном слое): {accuracy_output * 100:.2f}%\")\n",
        "print(f\"Точность (Сигмоида на всех слоях): {accuracy_sigmoid_all * 100:.2f}%\")\n",
        "print(f\"Время обучения (Softmax на всех слоях): {time_softmax_all:.4f} сек\")\n",
        "print(f\"Время обучения (Softmax только на выходном слое): {time_softmax_output:.4f} сек\")\n",
        "print(f\"Время обучения (Сигмоида на всех слоях): {time_sigmoid_all:.4f} сек\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4pF1mUuGXM9",
        "outputId": "3be8bc0a-bd72-4477-c47a-75fe2d97396d"
      },
      "id": "R4pF1mUuGXM9",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность (Softmax на всех слоях): 75.00%\n",
            "Точность (Softmax только на выходном слое): 75.00%\n",
            "Точность (Сигмоида на всех слоях): 100.00%\n",
            "Время обучения (Softmax на всех слоях): 0.0996 сек\n",
            "Время обучения (Softmax только на выходном слое): 0.0531 сек\n",
            "Время обучения (Сигмоида на всех слоях): 0.0445 сек\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты по точности и производительности всегда разные при различных величинах эпох и скорости обучения.\n",
        "Softmax применяется только выходном слое, если использовать на всех слоях, то можно столкнуться с замедлением обучения, нестабильностью градиентов и ухудшением качества модели.\n",
        "\n"
      ],
      "metadata": {
        "id": "sGqU3GWfIVb4"
      },
      "id": "sGqU3GWfIVb4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
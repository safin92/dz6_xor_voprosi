{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safin92/dz6_xor_voprosi/blob/main/XOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e86c20",
      "metadata": {
        "id": "09e86c20"
      },
      "source": [
        "### –ó–∞–¥–∞—á–∞ XOR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "953523e1",
      "metadata": {
        "id": "953523e1"
      },
      "source": [
        "# –ó–∞–¥–∞—á–∞: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã XOR\n",
        "\n",
        "## –í–≤–µ–¥–µ–Ω–∏–µ\n",
        "\n",
        "XOR (–∏—Å–∫–ª—é—á–∞—é—â–µ–µ –ò–õ–ò) - —ç—Ç–æ –ª–æ–≥–∏—á–µ—Å–∫–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–∏–Ω—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã. –¢–∞–±–ª–∏—Ü–∞ –∏—Å—Ç–∏–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è XOR:\n",
        "\n",
        "| A | B | A XOR B |\n",
        "|:-:|:-:|:-------:|\n",
        "| 0 | 0 |    0    |\n",
        "| 0 | 1 |    1    |\n",
        "| 1 | 0 |    1    |\n",
        "| 1 | 1 |    0    |\n",
        "\n",
        "–≠—Ç–∞ –∑–∞–¥–∞—á–∞ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–µ—à–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é –ª–∏–Ω–µ–π–Ω–æ–π –º–æ–¥–µ–ª–∏, –ø–æ—ç—Ç–æ–º—É –æ–Ω–∞ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.\n",
        "\n",
        "## –ó–∞–¥–∞–Ω–∏–µ\n",
        "\n",
        "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ - —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–≤—É—Ö—Å–ª–æ–π–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã XOR.\n",
        "\n",
        "### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏:\n",
        "\n",
        "- –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: 2 –Ω–µ–π—Ä–æ–Ω–∞ (–¥–ª—è A –∏ B)\n",
        "- –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π: 4 –Ω–µ–π—Ä–æ–Ω–∞\n",
        "- –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: 1 –Ω–µ–π—Ä–æ–Ω\n",
        "\n",
        "![–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è XOR](Tutorial-And.jpg)\n",
        "\n",
        "## –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n",
        "\n",
        "1. –ò–∑—É—á–∏—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥ –∫–ª–∞—Å—Å–∞ `NeuralNetwork`.\n",
        "2. –ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –º–µ—Å—Ç–∞, –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ `TODO`, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏.\n",
        "3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏ NumPy (`np.dot`, `np.random.uniform`, `np.sum`) –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.\n",
        "4. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –ø—Ä—è–º–æ–µ –∏ –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ, –∞ —Ç–∞–∫–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤.\n",
        "5. –û–±—É—á–∏—Ç–µ —Å–µ—Ç—å –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö XOR.\n",
        "6. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ —Å–µ—Ç—å –∏ –≤—ã–≤–µ–¥–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "274ade8a",
      "metadata": {
        "id": "274ade8a"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "25c459f9",
      "metadata": {
        "id": "25c459f9"
      },
      "outputs": [],
      "source": [
        "# –ö–ª–∞—Å—Å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å –æ–¥–Ω–∏–º —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–µ—Ç–∏\n",
        "        self.input_size = input_size      # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ (–ø—Ä–∏–∑–Ω–∞–∫–æ–≤))\n",
        "        self.hidden_size = hidden_size    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–∫—Ä—ã—Ç–æ–º —Å–ª–æ–µ\n",
        "        self.output_size = output_size    # –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        # –®–∞–≥ 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        # –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –∏ —Å–º–µ—â–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –¥–ª—è —Ä–∞–∑—Ä—ã–≤–∞ —Å–∏–º–º–µ—Ç—Ä–∏–∏ –≤ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size)) # –í–µ—Å–∞ –º–µ–∂–¥—É –≤—Ö–æ–¥–Ω—ã–º –∏ —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))              # –°–º–µ—â–µ–Ω–∏—è –¥–ª—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size)) # –í–µ—Å–∞ –º–µ–∂–¥—É —Å–∫—Ä—ã—Ç—ã–º –∏ –≤—ã—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))              # –°–º–µ—â–µ–Ω–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "    # –®–∞–≥ 2: –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ - –°–∏–≥–º–æ–∏–¥–∞\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏ –≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # –®–∞–≥ 3: –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Å–∏–≥–º–æ–∏–¥—ã\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –æ–±—Ä–∞—Ç–Ω–æ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–∏ –æ—à–∏–±–∫–∏\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # –®–∞–≥ 4: –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ (Forward Propagation)\n",
        "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å–µ—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    def forward(self, X):\n",
        "        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π -> –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è, –∑–∞—Ç–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        self.hidden_layer = self.sigmoid(np.dot(X, self.hidden_weights) + self.hidden_bias)\n",
        "\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π -> –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è, –∑–∞—Ç–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        self.output_layer = self.sigmoid(np.dot(self.hidden_layer, self.output_weights) + self.output_bias)\n",
        "\n",
        "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—ã—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
        "        return self.output_layer\n",
        "\n",
        "    # –®–∞–≥ 5: –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (Backward Propagation)\n",
        "    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    def backward(self, X, y, output):\n",
        "        # –®–∞–≥ 5.1: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        # –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
        "        error = y - output\n",
        "\n",
        "        # –®–∞–≥ 5.2: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        # –£–º–Ω–æ–∂–∞–µ–º –æ—à–∏–±–∫—É –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        d_output = error * self.sigmoid_derivative(output)\n",
        "\n",
        "        # –®–∞–≥ 5.3: –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –Ω–∞ —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π\n",
        "        # –£–º–Ω–æ–∂–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è –Ω–∞ –≤–µ—Å–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # –®–∞–≥ 5.4: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
        "        # –£–º–Ω–æ–∂–∞–µ–º –æ—à–∏–±–∫—É —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer)\n",
        "\n",
        "        # –®–∞–≥ 5.5: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
        "        self.output_weights += np.dot(self.hidden_layer.T, d_output) * self.learning_rate\n",
        "        self.output_bias += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π\n",
        "        self.hidden_weights += np.dot(X.T, d_hidden_layer) * self.learning_rate\n",
        "        self.hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "    # –®–∞–≥ 6: –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "    # –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —ç—Ç–∞–ø–æ–≤ –ø—Ä—è–º–æ–≥–æ –∏ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)        # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "            self.backward(X, y, output)     # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "\n",
        "    # –®–∞–≥ 7: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–∞ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a3cec421",
      "metadata": {
        "id": "a3cec421"
      },
      "outputs": [],
      "source": [
        "# –ù–∞—à \"–¥–∞—Ç–∞—Å–µ—Ç\" –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1744a778",
      "metadata": {
        "id": "1744a778"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "#–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
        "nn.train(X, y, epochs = 1000, learning_rate=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0114af1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0114af1f",
        "outputId": "e176640b-3ee1-45b9-bf23-62c0265ca93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\n",
            "[[0.99997724]\n",
            " [0.99999443]\n",
            " [0.99999296]\n",
            " [0.99999706]]\n",
            "\n",
            "–¢–æ—á–Ω–æ—Å—Ç—å: 50.0%\n"
          ]
        }
      ],
      "source": [
        "predictions = nn.predict(X)\n",
        "print(\"–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\")\n",
        "print(predictions)\n",
        "\n",
        "# –ü–æ–¥—Å–∫–∞–∑–∫–∞: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ np.mean –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "# np.mean –≤—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –º–∞—Å—Å–∏–≤–∞\n",
        "accuracy = np.mean(np.round(predictions) == y)\n",
        "print(f\"\\n–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy * 100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jlxFefldpazc"
      },
      "id": "jlxFefldpazc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. –ö–∞–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–µ–π—Ä–æ–Ω–Ω—ã–π —Å–µ—Ç–µ–π.**\n",
        "\n",
        "–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (–∏–ª–∏ –æ–±—Ä—Ç–∞–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏). –≠–ª–µ–º–µ–Ω—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è:\n",
        "  \n",
        "  1) –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ: –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ —Å–µ—Ç—å, –æ—Ç –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É - –∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω –≤–∑–≤–µ—à–∏–≤–∞–µ—Ç —Å—É–º–º—É –≤—Ö–æ–¥–æ–≤, –¥–æ–±–∞–≤–ª–µ—Ç —Å–º–µ—â–µ–Ω–∏–µ –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ - –Ω–∞ –≤—ã—Ö–æ–¥–µ —Å–µ—Ç—å –¥–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.\n",
        "  \n",
        "  2) –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–µ—Ç–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –∏ –≤—ã—á–∏—Å–ª—è–µ–º –æ—à–∏–±–∫—É (MSE, –∫—Ä–æ—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è).\n",
        "  \n",
        "  3) –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏: –æ—à–∏–±–∫–∞ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞–∑–∞–¥ –ø–æ —Å–µ—Ç–∏, –Ω–∞—á–∏–Ω–∞—è —Å –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è - –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (–ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ –≤—Å–µ–º –≤–µ—Å–∞–º).\n",
        "  \n",
        "  4) –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤: –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –≤–µ—Å–∞ —Ç–∞–∫, —á—Ç–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å –æ—à–∏–±–∫—É - –≤–µ—Å –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø–æ —Ñ–æ—Ä–º—É–ª–µ (–Ω–æ–≤—ã–π –≤–µ—Å = —Å—Ç–∞—Ä—ã–π –≤–µ—Å - —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è*–≥—Ä–∞–¥–∏–µ–Ω—Ç –æ—à–∏–±–∫–∏ –ø–æ –≤–µ—Å—É.\n",
        "\n",
        "\n",
        "**2. –ü–æ –∫–∞–∫–æ–º—É –º–µ—Ç–æ–¥—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∞–ª–≥–æ—Ä–∏—Ç–º?**\n",
        "\n",
        "–ú–µ—Ç–æ–¥—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞:\n",
        "  \n",
        "  1) –ü–æ–ª–Ω—ã–π (Batch) –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (Batch Gradient Descent, BGD)\n",
        "\n",
        "–û–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞ —Ä–∞–∑ –≤ —ç–ø–æ—Ö—É (–ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –ø–æ—Å–µ—Ç–∏), –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å—é –≤—ã–±–æ—Ä–∫—É –¥–∞–Ω–Ω—ã—Ö.\n",
        "–¢–æ—á–Ω—ã–π, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω—ã–π –∏ —Ç—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏.\n",
        "–•–æ—Ä–æ—à –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏).\n",
        "\n",
        "  2) –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (Stochastic Gradient Descent, SGD)\n",
        "\n",
        "–û–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ (–ø—Ä–∏–º–µ—Ä - –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤—Ö–æ–¥, –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç –¥–∞–Ω–Ω—ã—Ö , –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥–∞–µ—Ç—Å—è –Ω–∞ –≤—Ö–æ–¥ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏).\n",
        "–ë—ã—Å—Ç—Ä–µ–µ, –Ω–æ –∏–∑-–∑–∞ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –º–æ–∂–µ—Ç –∫–æ–ª–µ–±–∞—Ç—å—Å—è –∏ –Ω–µ –≤—Å–µ–≥–¥–∞ —Å—Ö–æ–¥–∏—Ç—Å—è –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É –º–∏–Ω–∏–º—É–º—É.\n",
        "–•–æ—Ä–æ—à –¥–ª—è –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è.\n",
        "\n",
        "  3) –ú–∏–Ω–∏-–±–∞—Ç—á –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (Mini-Batch Gradient Descent, MBGD)\n",
        "\n",
        "–ö–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É BGD –∏ SGD. –†–∞–∑–±–∏–≤–∞–µ—Ç –≤—ã–±–æ—Ä–∫—É –Ω–∞ –º–∞–ª–µ–Ω—å–∫–∏–µ –≥—Ä—É–ø–ø—ã (–±–∞—Ç—á–∏) –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞.\n",
        "–ë—ã—Å—Ç—Ä–µ–µ, —á–µ–º BGD, –∏ –º–µ–Ω–µ–µ —à—É–º–Ω—ã–π, —á–µ–º SGD.\n",
        "\n",
        "  4) Adagrad (Adaptive Gradient Descent)\n",
        "\n",
        "–†–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–∞—é—Ç—Å—è —Å —Ä–∞–∑–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é.\n",
        "–•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
        "–ü—Ä–æ–±–ª–µ–º–∞: —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–π.\n",
        "\n",
        "  5) RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "–£–ª—É—á—à–∞–µ—Ç Adagrad: –¥–æ–±–∞–≤–ª—è–µ—Ç –∑–∞—Ç—É—Ö–∞—é—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.\n",
        "–•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö.\n",
        "\n",
        "  6) Adam (Adaptive Moment Estimation) ‚Äì —Å–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π\n",
        "\n",
        "–ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç Momentum –∏ RMSprop.\n",
        "–°–∞–º—ã–π —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö.\n",
        "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞.\n",
        "\n",
        "–ö–∞–∫–æ–π –º–µ—Ç–æ–¥ –≤—ã–±—Ä–∞—Ç—å?\n",
        "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ ‚Üí BGD (–ø–æ–ª–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫).\n",
        "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ ‚Üí Mini-Batch –∏–ª–∏ Adam.\n",
        "–î–ª—è –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è ‚Üí SGD.\n",
        "–î–ª—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π ‚Üí Adam –∏–ª–∏ RMSprop.\n",
        "\n",
        "–í –∫–æ–¥–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –ø–æ–ª–Ω—ã–π (Batch) –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫, —Ç–∞–∫ –∫–∞–∫ –≤–µ—Å–∞ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.\n",
        "\n",
        "**3. –ö–∞–∫ –ø–æ–¥–±–∏—Ä–∞—Ç—å Learning_rate**\n",
        "\n",
        "–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤–µ—Å–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –æ–±—É—á–µ–Ω–∏—è.\n",
        "\n",
        "–í—ã–±–æ—Ä learning_rate –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω:\n",
        "\n",
        "–°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π Œ± ‚Üí –æ–±—É—á–µ–Ω–∏–µ –∏–¥–µ—Ç –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ, –º–æ–∂–Ω–æ –∑–∞—Å—Ç—Ä—è—Ç—å –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –º–∏–Ω–∏–º—É–º–µ.\n",
        "–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π Œ± ‚Üí –≤–µ—Å–∞ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —Å–ª–∏—à–∫–æ–º —Ä–µ–∑–∫–æ, –º–æ–∂–µ—Ç –Ω–∞—á–∞—Ç—å—Å—è \"—Å–∫–∞—á–∫–æ–æ–±—Ä–∞–∑–Ω–æ–µ\" –æ–±—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–µ—Ç—å –≤–æ–æ–±—â–µ –Ω–µ —Å–æ–π–¥–µ—Ç—Å—è.\n",
        "\n",
        "–ú–µ—Ç–æ–¥—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞:\n",
        "\n",
        "1) –ú–µ—Ç–æ–¥ –ø—Ä–æ–± –∏ –æ—à–∏–±–æ–∫ (—ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫)\n",
        "–ü—Ä–æ—Å—Ç–µ–π—à–∏–π —Å–ø–æ—Å–æ–± ‚Äî –Ω–∞—á–∞—Ç—å —Å –Ω–µ–±–æ–ª—å—à–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è (0.01 –∏–ª–∏ 0.001) –∏ –Ω–∞–±–ª—é–¥–∞—Ç—å –∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –æ—à–∏–±–∫–∏.\n",
        "\n",
        "–ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –º–µ–¥–ª–µ–Ω–Ω–æ ‚Üí –ø–æ–ø—Ä–æ–±—É–π —É–≤–µ–ª–∏—á–∏—Ç—å Œ±.\n",
        "–ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —Å–∫–∞—á–µ—Ç –∏–ª–∏ —Ä–∞—Å—Ç–µ—Ç ‚Üí —É–º–µ–Ω—å—à–∞–π Œ±.\n",
        "\n",
        "–ü—Ä–∏–º–µ—Ä:\n",
        "\n",
        "nn.train(X, y, epochs=1000, learning_rate=0.01)\n",
        "\n",
        "–ù–∞—á–Ω–∏ —Å 0.01. –ï—Å–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç \"—Å–∫–∞—á–µ—Ç\" ‚Üí —É–º–µ–Ω—å—à–∏ Œ± (0.001).\n",
        "–ï—Å–ª–∏ —Å—Ö–æ–¥–∏—Ç—Å—è –º–µ–¥–ª–µ–Ω–Ω–æ ‚Üí –ø–æ–ø—Ä–æ–±—É–π 0.05 –∏–ª–∏ 0.1.\n",
        "\n",
        "2) –ú–µ—Ç–æ–¥ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–ø–æ–∏—Å–∫ –ø–æ —Å—Ç–µ–ø–µ–Ω—è–º 10)\n",
        "–ü–æ–ø—Ä–æ–±—É–π —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è Œ± –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 1e-5 –¥–æ 1e-1 (–æ—Ç 0.00001 –¥–æ 0.1):\n",
        "\n",
        "    for lr in [0.00001, 0.0001, 0.001, 0.01, 0.1]:\n",
        "\n",
        "    print(f\"–¢–µ—Å—Ç–∏—Ä—É–µ–º learning_rate = {lr}\")\n",
        "\n",
        "    nn.train(X, y, epochs=1000, learning_rate=lr)\n",
        "    \n",
        "–ï—Å–ª–∏ Œ± = 0.1 —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ, –ø–æ–ø—Ä–æ–±—É–π 0.05 –∏–ª–∏ 0.2 –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è.\n",
        "\n",
        "3) –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã (Adam, RMSprop, Adagrad)\n",
        "–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø–æ–¥—Å—Ç—Ä–æ–π–∫—É learning_rate:\n",
        "\n",
        "Adam ‚Äî –æ–¥–∏–Ω –∏–∑ –ª—É—á—à–∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ (—Å–æ—á–µ—Ç–∞–µ—Ç Momentum + RMSprop).\n",
        "RMSprop ‚Äî —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.\n",
        "Adagrad ‚Äî –ø–æ–¥—Ö–æ–¥–∏—Ç, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–∏–ª—å–Ω–æ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –ø–æ –º–∞—Å—à—Ç–∞–±–∞–º.\n",
        "\n",
        "–ü—Ä–∏–º–µ—Ä —Å Adam –≤ Keras:\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "\n",
        "4) –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å \"learning rate decay\" (—É–º–µ–Ω—å—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º)\n",
        "–ò–Ω–æ–≥–¥–∞ –æ–±—É—á–µ–Ω–∏–µ —Å–Ω–∞—á–∞–ª–∞ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–æ–≥–æ Œ±, –Ω–æ –ø–æ—Ç–æ–º –µ–≥–æ –ª—É—á—à–µ —É–º–µ–Ω—å—à–∏—Ç—å. –≠—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"learning rate decay\" (–∑–∞—Ç—É—Ö–∞–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è).\n",
        "\n",
        "–ü—Ä–∏–º–µ—Ä:\n",
        "\n",
        "    initial_lr = 0.1\n",
        "    decay_rate = 0.01\n",
        "    lr = initial_lr / (1 + decay_rate * epoch)  # –£–º–µ–Ω—å—à–∞–µ–º lr –ø–æ –º–µ—Ä–µ —Ä–æ—Å—Ç–∞ —ç–ø–æ—Ö\n",
        "\n",
        "–ß–µ–º –¥–∞–ª—å—à–µ –æ–±—É—á–µ–Ω–∏–µ, —Ç–µ–º –º–µ–Ω—å—à–µ —à–∞–≥–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç —Ç–æ—á–Ω–µ–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–µ—Ç—å.\n",
        "\n",
        "5) –í–∏–∑—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–≥—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–∫–∏)\n",
        "–ú–æ–∂–Ω–æ —Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–∫–∏ (loss function) –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —ç–ø–æ—Ö.\n",
        "\n",
        "–ü—Ä–∏–º–µ—Ä:\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    losses = []\n",
        "    for epoch in range(1000):\n",
        "      output = nn.forward(X)\n",
        "      loss = np.mean((y - output) ** 2)  # –°—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞ (MSE)\n",
        "      losses.append(loss)\n",
        "    nn.backward(X, y, output)\n",
        "\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel(\"–≠–ø–æ—Ö–∏\")\n",
        "    plt.ylabel(\"–û—à–∏–±–∫–∞ (loss)\")\n",
        "    plt.show()\n",
        "\n",
        "–ï—Å–ª–∏ –∫—Ä–∏–≤–∞—è –ø–∞–¥–∞–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ ‚Üí —É–≤–µ–ª–∏—á—å Œ±.\n",
        "–ï—Å–ª–∏ —Å–∫–∞—á–µ—Ç –∏–ª–∏ —Ä–∞—Å—Ç–µ—Ç ‚Üí —É–º–µ–Ω—å—à–∏ Œ±.\n",
        "\n",
        "–í—ã–≤–æ–¥:\n",
        "–ù–∞—á–∏–Ω–∞–π —Å Œ± = 0.01 –∏–ª–∏ 0.001, —Ç–µ—Å—Ç–∏—Ä—É–π.\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (0.00001 ‚Üí 0.0001 ‚Üí 0.001 ‚Üí 0.01 ‚Üí 0.1).\n",
        "–î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–ø—Ä–æ–±—É–π Adam –∏–ª–∏ RMSprop –≤–º–µ—Å—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞.\n",
        "–ú–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∞—Ç—å Œ± —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º (learning rate decay).\n",
        "–°—Ç—Ä–æ–π –≥—Ä–∞—Ñ–∏–∫–∏ –æ—à–∏–±–∫–∏, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å, –∫–∞–∫ Œ± –≤–ª–∏—è–µ—Ç –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ.\n",
        "–õ—É—á—à–∏–π learning_rate –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–∞–Ω–Ω—ã—Ö! üöÄ\n",
        "\n",
        "**4. –ß—Ç–æ —Ç–∞–∫–æ–µ np.random.uniform?**\n",
        "\n",
        "np.random ‚Äî —ç—Ç–æ –º–æ–¥—É–ª—å –≤ NumPy, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª.\n",
        "\n",
        "np.random.uniform(low, high, size) ‚Äî —ç—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ np.random, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —á–∏—Å–ª–∞ –≤ –∑–∞–¥–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ [low, high]."
      ],
      "metadata": {
        "id": "cQuICl6ZgkA9"
      },
      "id": "cQuICl6ZgkA9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5) –≤–µ—Å–∞ –∑–∞–¥–∞—Ç—å –Ω—É–ª—è–º–∏ –∏ —Å–º–µ—à–µ–Ω–∏—è —Ç–æ–∂–µ**\n",
        "\n",
        "–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫–æ–¥ —Å –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏ —Ä–∞–≤–Ω—ã–º–∏ 0.\n"
      ],
      "metadata": {
        "id": "lNIiLFxI_D2k"
      },
      "id": "lNIiLFxI_D2k"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# –ö–ª–∞—Å—Å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å –æ–¥–Ω–∏–º —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–µ—Ç–∏\n",
        "        self.input_size = input_size      # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ (–ø—Ä–∏–∑–Ω–∞–∫–æ–≤))\n",
        "        self.hidden_size = hidden_size    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–∫—Ä—ã—Ç–æ–º —Å–ª–æ–µ\n",
        "        self.output_size = output_size    # –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        # –®–∞–≥ 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        # –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –∏ —Å–º–µ—â–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –¥–ª—è —Ä–∞–∑—Ä—ã–≤–∞ —Å–∏–º–º–µ—Ç—Ä–∏–∏ –≤ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö\n",
        "        self.hidden_weights = np.zeros((input_size, hidden_size)) # –í–µ—Å–∞ –º–µ–∂–¥—É –≤—Ö–æ–¥–Ω—ã–º –∏ —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º = 0\n",
        "        self.hidden_bias = np.zeros((1, hidden_size))              # –°–º–µ—â–µ–Ω–∏—è –¥–ª—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è = 0\n",
        "        self.output_weights = np.zeros((hidden_size, output_size)) # –í–µ—Å–∞ –º–µ–∂–¥—É —Å–∫—Ä—ã—Ç—ã–º –∏ –≤—ã—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º = 0\n",
        "        self.output_bias = np.zeros((1, output_size))              # –°–º–µ—â–µ–Ω–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è = 0\n",
        "\n",
        "    # –®–∞–≥ 2: –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ - –°–∏–≥–º–æ–∏–¥–∞\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏ –≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # –®–∞–≥ 3: –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Å–∏–≥–º–æ–∏–¥—ã\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –æ–±—Ä–∞—Ç–Ω–æ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–∏ –æ—à–∏–±–∫–∏\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # –®–∞–≥ 4: –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ (Forward Propagation)\n",
        "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å–µ—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    def forward(self, X):\n",
        "        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π -> –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è, –∑–∞—Ç–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        self.hidden_layer = self.sigmoid(np.dot(X, self.hidden_weights) + self.hidden_bias)\n",
        "\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π -> –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è, –∑–∞—Ç–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        self.output_layer = self.sigmoid(np.dot(self.hidden_layer, self.output_weights) + self.output_bias)\n",
        "\n",
        "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—ã—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
        "        return self.output_layer\n",
        "\n",
        "    # –®–∞–≥ 5: –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (Backward Propagation)\n",
        "    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    def backward(self, X, y, output):\n",
        "        # –®–∞–≥ 5.1: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        # –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
        "        error = y - output\n",
        "\n",
        "        # –®–∞–≥ 5.2: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        # –£–º–Ω–æ–∂–∞–µ–º –æ—à–∏–±–∫—É –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        d_output = error * self.sigmoid_derivative(output)\n",
        "\n",
        "        # –®–∞–≥ 5.3: –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –Ω–∞ —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π\n",
        "        # –£–º–Ω–æ–∂–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è –Ω–∞ –≤–µ—Å–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # –®–∞–≥ 5.4: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
        "        # –£–º–Ω–æ–∂–∞–µ–º –æ—à–∏–±–∫—É —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer)\n",
        "\n",
        "        # –®–∞–≥ 5.5: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
        "        self.output_weights += np.dot(self.hidden_layer.T, d_output) * self.learning_rate\n",
        "        self.output_bias += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π\n",
        "        self.hidden_weights += np.dot(X.T, d_hidden_layer) * self.learning_rate\n",
        "        self.hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "    # –®–∞–≥ 6: –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "    # –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —ç—Ç–∞–ø–æ–≤ –ø—Ä—è–º–æ–≥–æ –∏ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)        # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "            self.backward(X, y, output)     # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "\n",
        "    # –®–∞–≥ 7: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–∞ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# –ù–∞—à \"–¥–∞—Ç–∞—Å–µ—Ç\" –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "#–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
        "nn.train(X, y, epochs = 1000, learning_rate=10)\n",
        "\n",
        "predictions = nn.predict(X)\n",
        "print(\"–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\")\n",
        "print(predictions)\n",
        "\n",
        "# –ü–æ–¥—Å–∫–∞–∑–∫–∞: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ np.mean –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "# np.mean –≤—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –º–∞—Å—Å–∏–≤–∞\n",
        "accuracy = np.mean(np.round(predictions) == y)\n",
        "print(f\"\\n–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy4MepXN_LAc",
        "outputId": "4d9b04ae-3fb3-476f-c453-ca774890cfd1"
      },
      "id": "Yy4MepXN_LAc",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\n",
            "[[0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]]\n",
            "\n",
            "–¢–æ—á–Ω–æ—Å—Ç—å: 50.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ü—Ä–∏ –∑–∞–¥–∞–Ω–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–º–µ—â–µ–Ω–∏–π –Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Ä–∞—å–±–æ—Ç—É, –µ—Å–ª–∏ –∑–∞–¥–∞—Ç—å –∏ –≤–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è –Ω—É–ª—è–º–∏, —Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è.\n",
        "\n",
        "1) –í—Å–µ –Ω–µ–π—Ä–æ–Ω—ã –≤ –æ–¥–Ω–æ–º —Å–ª–æ–µ –ø–æ–ª—É—á–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã.\n",
        "–í–æ –≤—Ä–µ–º—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è (backpropagation) –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è —á–µ—Ä–µ–∑ –≤–µ—Å–∞. –ï—Å–ª–∏ –≤—Å–µ –≤–µ—Å–∞ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –Ω—É–ª–∏, —Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ –±—É–¥—É—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏.\n",
        "–≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ä–∞–∑—Ä—ã–≤—É —Å–∏–º–º–µ—Ç—Ä–∏–∏ ‚Äî –≤—Å–µ –Ω–µ–π—Ä–æ–Ω—ã –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ, –∏ —Å–µ—Ç—å –Ω–µ –º–æ–∂–µ—Ç —É—á–∏—Ç—å—Å—è.\n",
        "\n",
        "2) –°–µ—Ç—å –æ—Å—Ç–∞—ë—Ç—Å—è –ª–∏–Ω–µ–π–Ω–æ–π.\n",
        "–ë–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–∞ –æ—Å—Ç–∞—é—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏, –∞ –∑–Ω–∞—á–∏—Ç, –∫–∞–∂–¥—ã–π —Å–ª–æ–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ.\n",
        "–õ–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ —Å–ø–æ—Å–æ–±–Ω–∞ –æ–±—É—á–∞—Ç—å—Å—è —Å–ª–æ–∂–Ω—ã–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º (–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω—É–∂–Ω–∞ –∏–º–µ–Ω–Ω–æ –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è)."
      ],
      "metadata": {
        "id": "TWxHfKMmkcPu"
      },
      "id": "TWxHfKMmkcPu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. –í —à–∞–≥–µ 5.5 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è \"+=\", –ø–æ—á–µ–º—É –Ω–µ \"-+\"?**\n",
        "\n",
        "–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫:\n",
        "\n",
        "ùëä = ùëä ‚àí ùúÇ‚ãÖ‚àÇùêø\n",
        "\n",
        "–≥–¥–µ:\n",
        "\n",
        "ùëä ‚Äî –≤–µ—Å–∞,\n",
        "ùúÇ (learning rate) ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è,\n",
        "‚àÇùêø ‚Äî –≥—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –æ—à–∏–±–∫–∏ (L ‚Äî loss).\n",
        "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –æ–±—ã—á–Ω–æ –º—ã –≤—ã—á–∏—Ç–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç (-=), —á—Ç–æ–±—ã –¥–≤–∏–≥–∞—Ç—å—Å—è –≤ —Å—Ç–æ—Ä–æ–Ω—É —É–±—ã–≤–∞–Ω–∏—è –æ—à–∏–±–∫–∏.\n",
        "\n",
        "–ü—Ä–∞–≤–∏–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å -=, –Ω–æ –≤ –∫–æ–¥–µ —É–∂–µ –∏–º–µ–µ—Ç—Å—è –æ—à–∏–±–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–ª—é—Å - error = y - output. –ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å -=, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª—É error = y - output –Ω–∞ error = output - y\n",
        "\n"
      ],
      "metadata": {
        "id": "nbblFcts_Jj9"
      },
      "id": "nbblFcts_Jj9"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# –ö–ª–∞—Å—Å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å –æ–¥–Ω–∏–º —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–µ—Ç–∏\n",
        "        self.input_size = input_size      # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ (–ø—Ä–∏–∑–Ω–∞–∫–æ–≤))\n",
        "        self.hidden_size = hidden_size    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–∫—Ä—ã—Ç–æ–º —Å–ª–æ–µ\n",
        "        self.output_size = output_size    # –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        # –®–∞–≥ 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        # –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –∏ —Å–º–µ—â–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –¥–ª—è —Ä–∞–∑—Ä—ã–≤–∞ —Å–∏–º–º–µ—Ç—Ä–∏–∏ –≤ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö\n",
        "        self.hidden_weights = np.random.uniform(size = (input_size, hidden_size)) # –í–µ—Å–∞ –º–µ–∂–¥—É –≤—Ö–æ–¥–Ω—ã–º –∏ —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º\n",
        "        self.hidden_bias = np.random.uniform(size = (1, hidden_size))              # –°–º–µ—â–µ–Ω–∏—è –¥–ª—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
        "        self.output_weights = np.random.uniform(size = (hidden_size, output_size)) # –í–µ—Å–∞ –º–µ–∂–¥—É —Å–∫—Ä—ã—Ç—ã–º –∏ –≤—ã—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º\n",
        "        self.output_bias = np.random.uniform(size = (1, output_size))              # –°–º–µ—â–µ–Ω–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "    # –®–∞–≥ 2: –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ - –°–∏–≥–º–æ–∏–¥–∞\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏ –≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # –®–∞–≥ 3: –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Å–∏–≥–º–æ–∏–¥—ã\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –æ–±—Ä–∞—Ç–Ω–æ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–∏ –æ—à–∏–±–∫–∏\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # –®–∞–≥ 4: –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ (Forward Propagation)\n",
        "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å–µ—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    def forward(self, X):\n",
        "        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π -> –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è, –∑–∞—Ç–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        self.hidden_layer = self.sigmoid(np.dot(X, self.hidden_weights) + self.hidden_bias)\n",
        "\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π -> –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è, –∑–∞—Ç–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        self.output_layer = self.sigmoid(np.dot(self.hidden_layer, self.output_weights) + self.output_bias)\n",
        "\n",
        "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—ã—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
        "        return self.output_layer\n",
        "\n",
        "    # –®–∞–≥ 5: –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (Backward Propagation)\n",
        "    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    def backward(self, X, y, output):\n",
        "        # –®–∞–≥ 5.1: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        # –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
        "        error = output - y\n",
        "\n",
        "        # –®–∞–≥ 5.2: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        # –£–º–Ω–æ–∂–∞–µ–º –æ—à–∏–±–∫—É –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        d_output = error * self.sigmoid_derivative(output)\n",
        "\n",
        "        # –®–∞–≥ 5.3: –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –Ω–∞ —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π\n",
        "        # –£–º–Ω–æ–∂–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è –Ω–∞ –≤–µ—Å–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # –®–∞–≥ 5.4: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
        "        # –£–º–Ω–æ–∂–∞–µ–º –æ—à–∏–±–∫—É —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer)\n",
        "\n",
        "        # –®–∞–≥ 5.5: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
        "        self.output_weights -= np.dot(self.hidden_layer.T, d_output) * self.learning_rate\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "    # –®–∞–≥ 6: –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "    # –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —ç—Ç–∞–ø–æ–≤ –ø—Ä—è–º–æ–≥–æ –∏ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)        # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "            self.backward(X, y, output)     # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "\n",
        "    # –®–∞–≥ 7: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–∞ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# –ù–∞—à \"–¥–∞—Ç–∞—Å–µ—Ç\" –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "#–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
        "nn.train(X, y, epochs = 1000, learning_rate=10)\n",
        "\n",
        "predictions = nn.predict(X)\n",
        "print(\"–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\")\n",
        "print(predictions)\n",
        "\n",
        "# –ü–æ–¥—Å–∫–∞–∑–∫–∞: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ np.mean –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "# np.mean –≤—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –º–∞—Å—Å–∏–≤–∞\n",
        "accuracy = np.mean(np.round(predictions) == y)\n",
        "print(f\"\\n–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsewcjuOt1VJ",
        "outputId": "9885188d-f140-4f07-b00a-ab26ed254dac"
      },
      "id": "jsewcjuOt1VJ",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\n",
            "[[0.00979599]\n",
            " [0.98621319]\n",
            " [0.99202569]\n",
            " [0.01480533]]\n",
            "\n",
            "–¢–æ—á–Ω–æ—Å—Ç—å: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–æ—Ñ—Ç–º–∞–∫—Å –≤–º–µ—Å—Ç–æ —Å–∏–≥–º–æ–∏–¥—ã, –æ–±—É—á–∏—Ç—å —Å–æ—Ñ—Ç–º–∞–∫—Å–æ–º**\n",
        "\n",
        "–í–∞—Ä–∏–∞–Ω—Ç –ø–æ–ª–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Softmax"
      ],
      "metadata": {
        "id": "rqeCLLOVtpJ-"
      },
      "id": "rqeCLLOVtpJ-"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–µ—Ç–∏\n",
        "        self.input_size = input_size      # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        self.hidden_size = hidden_size    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–∫—Ä—ã—Ç–æ–º —Å–ª–æ–µ\n",
        "        self.output_size = output_size    # –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size))  # –í–µ—Å–∞ –º–µ–∂–¥—É –≤—Ö–æ–¥–Ω—ã–º –∏ —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))              # –°–º–µ—â–µ–Ω–∏—è –¥–ª—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size)) # –í–µ—Å–∞ –º–µ–∂–¥—É —Å–∫—Ä—ã—Ç—ã–º –∏ –≤—ã—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))              # –°–º–µ—â–µ–Ω–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "    # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ - Softmax\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è Softmax\n",
        "    def softmax_derivative(self, x):\n",
        "        s = x.reshape(-1, 1)\n",
        "        return np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ (Forward Propagation)\n",
        "    def forward(self, X):\n",
        "        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π -> –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π\n",
        "        self.hidden_layer_input = np.dot(X, self.hidden_weights) + self.hidden_bias\n",
        "        self.hidden_layer_output = self.softmax(self.hidden_layer_input)\n",
        "\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π -> –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.output_weights) + self.output_bias\n",
        "        self.output_layer_output = self.softmax(self.output_layer_input)\n",
        "\n",
        "        return self.output_layer_output\n",
        "\n",
        "    # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (Backward Propagation)\n",
        "    def backward(self, X, y, output):\n",
        "        batch_size = X.shape[0]\n",
        "\n",
        "        # –û—à–∏–±–∫–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        error = output - y\n",
        "\n",
        "        # –ì—Ä–∞–¥–∏–µ–Ω—Ç –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        d_output = np.zeros_like(output)\n",
        "        for i in range(batch_size):\n",
        "            d_output[i] = np.dot(self.softmax_derivative(output[i]), error[i])\n",
        "\n",
        "        # –û—à–∏–±–∫–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # –ì—Ä–∞–¥–∏–µ–Ω—Ç —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
        "        d_hidden_layer = np.zeros_like(self.hidden_layer_output)\n",
        "        for i in range(batch_size):\n",
        "            d_hidden_layer[i] = np.dot(self.softmax_derivative(self.hidden_layer_output[i]), error_hidden_layer[i])\n",
        "\n",
        "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        self.output_weights -= np.dot(self.hidden_layer_output.T, d_output) * self.learning_rate / batch_size\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate / batch_size\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "    # –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)        # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "            self.backward(X, y, output)     # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])  # One-hot encoding –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=2)\n",
        "\n",
        "# –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
        "nn.train(X, y, epochs=1000, learning_rate=10)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "predictions = nn.predict(X)\n",
        "print(\"–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\")\n",
        "print(predictions)\n",
        "\n",
        "# –¢–æ—á–Ω–æ—Å—Ç—å\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(y, axis=1)\n",
        "accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f\"\\n–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06fHwxMD0Emf",
        "outputId": "c66ed3ff-e742-452c-845e-ce6f4cd0378a"
      },
      "id": "06fHwxMD0Emf",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\n",
            "[[0.99344673 0.00655327]\n",
            " [0.00792608 0.99207392]\n",
            " [0.00782483 0.99217517]\n",
            " [0.99117347 0.00882653]]\n",
            "\n",
            "–¢–æ—á–Ω–æ—Å—Ç—å: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–º —Å–ª–æ–µ"
      ],
      "metadata": {
        "id": "nnbVOzyN6b8X"
      },
      "id": "nnbVOzyN6b8X"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–µ—Ç–∏\n",
        "        self.input_size = input_size      # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        self.hidden_size = hidden_size    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–∫—Ä—ã—Ç–æ–º —Å–ª–æ–µ\n",
        "        self.output_size = output_size    # –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size))  # –í–µ—Å–∞ –º–µ–∂–¥—É –≤—Ö–æ–¥–Ω—ã–º –∏ —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))              # –°–º–µ—â–µ–Ω–∏—è –¥–ª—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size)) # –í–µ—Å–∞ –º–µ–∂–¥—É —Å–∫—Ä—ã—Ç—ã–º –∏ –≤—ã—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))              # –°–º–µ—â–µ–Ω–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "    # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ - Softmax\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ - –°–∏–≥–º–æ–∏–¥–∞\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Å–∏–≥–º–æ–∏–¥—ã\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ (Forward Propagation)\n",
        "    def forward(self, X):\n",
        "        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π -> –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π\n",
        "        self.hidden_layer_input = np.dot(X, self.hidden_weights) + self.hidden_bias\n",
        "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π -> –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.output_weights) + self.output_bias\n",
        "        self.output_layer_output = self.softmax(self.output_layer_input)\n",
        "\n",
        "        return self.output_layer_output\n",
        "\n",
        "    # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (Backward Propagation)\n",
        "    def backward(self, X, y, output):\n",
        "        batch_size = X.shape[0]\n",
        "\n",
        "        # –û—à–∏–±–∫–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        error = output - y\n",
        "\n",
        "        # –ì—Ä–∞–¥–∏–µ–Ω—Ç –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        d_output = error  # –î–ª—è Softmax –≥—Ä–∞–¥–∏–µ–Ω—Ç —Ä–∞–≤–µ–Ω —Ä–∞–∑–Ω–∏—Ü–µ –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –∏ —Ü–µ–ª–µ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
        "\n",
        "        # –û—à–∏–±–∫–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # –ì—Ä–∞–¥–∏–µ–Ω—Ç —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        self.output_weights -= np.dot(self.hidden_layer_output.T, d_output) * self.learning_rate / batch_size\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate / batch_size\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "    # –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)        # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "            self.backward(X, y, output)     # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])  # One-hot encoding –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=2)\n",
        "\n",
        "# –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
        "nn.train(X, y, epochs=1000, learning_rate=10)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "predictions = nn.predict(X)\n",
        "print(\"–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\")\n",
        "print(predictions)\n",
        "\n",
        "# –¢–æ—á–Ω–æ—Å—Ç—å\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(y, axis=1)\n",
        "accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f\"\\n–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN6jWLUk6geK",
        "outputId": "ae88c44d-02e7-48bc-8f03-ff7ccc8bf6f8"
      },
      "id": "GN6jWLUk6geK",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:\n",
            "[[9.99177783e-01 8.22216852e-04]\n",
            " [5.13253049e-04 9.99486747e-01]\n",
            " [6.44733945e-04 9.99355266e-01]\n",
            " [9.99374725e-01 6.25274536e-04]]\n",
            "\n",
            "–¢–æ—á–Ω–æ—Å—Ç—å: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
      ],
      "metadata": {
        "id": "1RrHmofU_rYL"
      },
      "id": "1RrHmofU_rYL"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# –ö–ª–∞—Å—Å —Å Softmax –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö\n",
        "class NeuralNetworkSoftmaxAllLayers:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size))\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size))\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def softmax_derivative(self, x):\n",
        "        s = x.reshape(-1, 1)\n",
        "        return np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden_layer_input = np.dot(X, self.hidden_weights) + self.hidden_bias\n",
        "        self.hidden_layer_output = self.softmax(self.hidden_layer_input)\n",
        "\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.output_weights) + self.output_bias\n",
        "        self.output_layer_output = self.softmax(self.output_layer_input)\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        batch_size = X.shape[0]\n",
        "        error = output - y\n",
        "        d_output = error\n",
        "\n",
        "        # –ì—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "\n",
        "        # –ì—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è (–≤—ã—á–∏—Å–ª—è–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –æ—Ç–¥–µ–ª—å–Ω–æ)\n",
        "        d_hidden_layer = np.zeros_like(self.hidden_layer_output)\n",
        "        for i in range(batch_size):\n",
        "            softmax_deriv = self.softmax_derivative(self.hidden_layer_output[i])\n",
        "            d_hidden_layer[i] = np.dot(error_hidden_layer[i], softmax_deriv)\n",
        "\n",
        "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π\n",
        "        self.output_weights -= np.dot(self.hidden_layer_output.T, d_output) * self.learning_rate / batch_size\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate / batch_size\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "\n",
        "# –ö–ª–∞—Å—Å —Å Softmax —Ç–æ–ª—å–∫–æ –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–º —Å–ª–æ–µ\n",
        "class NeuralNetworkSoftmaxOutputLayer:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size))\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size))\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden_layer_input = np.dot(X, self.hidden_weights) + self.hidden_bias\n",
        "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.output_weights) + self.output_bias\n",
        "        self.output_layer_output = self.softmax(self.output_layer_input)\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        batch_size = X.shape[0]\n",
        "        error = output - y\n",
        "        d_output = error\n",
        "\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        self.output_weights -= np.dot(self.hidden_layer_output.T, d_output) * self.learning_rate / batch_size\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate / batch_size\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "\n",
        "# –ö–ª–∞—Å—Å —Å —Å–∏–≥–º–æ–∏–¥–æ–π –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö\n",
        "class NeuralNetworkSigmoidAllLayers:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.hidden_weights = np.random.uniform(size=(input_size, hidden_size))\n",
        "        self.hidden_bias = np.random.uniform(size=(1, hidden_size))\n",
        "        self.output_weights = np.random.uniform(size=(hidden_size, output_size))\n",
        "        self.output_bias = np.random.uniform(size=(1, output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden_layer_input = np.dot(X, self.hidden_weights) + self.hidden_bias\n",
        "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.output_weights) + self.output_bias\n",
        "        self.output_layer_output = self.sigmoid(self.output_layer_input)\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        batch_size = X.shape[0]\n",
        "        error = output - y\n",
        "        d_output = error * self.sigmoid_derivative(output)\n",
        "\n",
        "        error_hidden_layer = np.dot(d_output, self.output_weights.T)\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        self.output_weights -= np.dot(self.hidden_layer_output.T, d_output) * self.learning_rate / batch_size\n",
        "        self.output_bias -= np.sum(d_output, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "        self.hidden_weights -= np.dot(X.T, d_hidden_layer) * self.learning_rate / batch_size\n",
        "        self.hidden_bias -= np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate / batch_size\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "\n",
        "# –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])  # One-hot encoding\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
        "epochs = 1000\n",
        "learning_rate = 10\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Ç–µ–π\n",
        "nn_softmax_all = NeuralNetworkSoftmaxAllLayers(input_size=2, hidden_size=4, output_size=2)\n",
        "nn_softmax_output = NeuralNetworkSoftmaxOutputLayer(input_size=2, hidden_size=4, output_size=2)\n",
        "nn_sigmoid_all = NeuralNetworkSigmoidAllLayers(input_size=2, hidden_size=4, output_size=2)\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ –∏ –∏–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏\n",
        "start_time = time.time()\n",
        "nn_softmax_all.train(X, y, epochs, learning_rate)\n",
        "time_softmax_all = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "nn_softmax_output.train(X, y, epochs, learning_rate)\n",
        "time_softmax_output = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "nn_sigmoid_all.train(X, y, epochs, learning_rate)\n",
        "time_sigmoid_all = time.time() - start_time\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å\n",
        "predictions_all = nn_softmax_all.predict(X)\n",
        "predictions_output = nn_softmax_output.predict(X)\n",
        "predictions_sigmoid_all = nn_sigmoid_all.predict(X)\n",
        "\n",
        "accuracy_all = np.mean(np.argmax(predictions_all, axis=1) == np.argmax(y, axis=1))\n",
        "accuracy_output = np.mean(np.argmax(predictions_output, axis=1) == np.argmax(y, axis=1))\n",
        "accuracy_sigmoid_all = np.mean(np.argmax(predictions_sigmoid_all, axis=1) == np.argmax(y, axis=1))\n",
        "\n",
        "# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "print(f\"–¢–æ—á–Ω–æ—Å—Ç—å (Softmax –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö): {accuracy_all * 100:.2f}%\")\n",
        "print(f\"–¢–æ—á–Ω–æ—Å—Ç—å (Softmax —Ç–æ–ª—å–∫–æ –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–º —Å–ª–æ–µ): {accuracy_output * 100:.2f}%\")\n",
        "print(f\"–¢–æ—á–Ω–æ—Å—Ç—å (–°–∏–≥–º–æ–∏–¥–∞ –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö): {accuracy_sigmoid_all * 100:.2f}%\")\n",
        "print(f\"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (Softmax –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö): {time_softmax_all:.4f} —Å–µ–∫\")\n",
        "print(f\"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (Softmax —Ç–æ–ª—å–∫–æ –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–º —Å–ª–æ–µ): {time_softmax_output:.4f} —Å–µ–∫\")\n",
        "print(f\"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–°–∏–≥–º–æ–∏–¥–∞ –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö): {time_sigmoid_all:.4f} —Å–µ–∫\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4pF1mUuGXM9",
        "outputId": "3be8bc0a-bd72-4477-c47a-75fe2d97396d"
      },
      "id": "R4pF1mUuGXM9",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–¢–æ—á–Ω–æ—Å—Ç—å (Softmax –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö): 75.00%\n",
            "–¢–æ—á–Ω–æ—Å—Ç—å (Softmax —Ç–æ–ª—å–∫–æ –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–º —Å–ª–æ–µ): 75.00%\n",
            "–¢–æ—á–Ω–æ—Å—Ç—å (–°–∏–≥–º–æ–∏–¥–∞ –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö): 100.00%\n",
            "–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (Softmax –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö): 0.0996 —Å–µ–∫\n",
            "–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (Softmax —Ç–æ–ª—å–∫–æ –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–º —Å–ª–æ–µ): 0.0531 —Å–µ–∫\n",
            "–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–°–∏–≥–º–æ–∏–¥–∞ –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö): 0.0445 —Å–µ–∫\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–¥–∞ —Ä–∞–∑–Ω—ã–µ –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–µ–ª–∏—á–∏–Ω–∞—Ö —ç–ø–æ—Ö –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.\n",
        "Softmax –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤—ã—Ö–æ–¥–Ω–æ–º —Å–ª–æ–µ, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞ –≤—Å–µ—Ö —Å–ª–æ—è—Ö, —Ç–æ –º–æ–∂–Ω–æ —Å—Ç–æ–ª–∫–Ω—É—Ç—å—Å—è —Å –∑–∞–º–µ–¥–ª–µ–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è, –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ —É—Ö—É–¥—à–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏.\n",
        "\n"
      ],
      "metadata": {
        "id": "sGqU3GWfIVb4"
      },
      "id": "sGqU3GWfIVb4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}